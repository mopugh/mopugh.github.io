[{"authors":["admin"],"categories":null,"content":"Matthew Pugh is a principal member of the technical staff at Sandia National Laboratories in Livermore, California. His early career focused on research in sensor and signal processing, algorithm design and communications. More recently he leads teams of engineers to execute critical laboratory programs.\n\u0026ldquo;The purpose of computation is insight, not numbers.\u0026rdquo;\n\u0026#8212; Richard Hamming\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"http://rexyroo.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Matthew Pugh is a principal member of the technical staff at Sandia National Laboratories in Livermore, California. His early career focused on research in sensor and signal processing, algorithm design and communications. More recently he leads teams of engineers to execute critical laboratory programs.\n\u0026ldquo;The purpose of computation is insight, not numbers.\u0026rdquo;\n\u0026#8212; Richard Hamming","tags":null,"title":"Matthew Pugh","type":"authors"},{"authors":["Matthew Pugh","Jerry Brewer","Jacques Kvam"],"categories":[],"content":"","date":1426988722,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1426988722,"objectID":"45992f529a3a713085e9315666b2cf69","permalink":"http://rexyroo.github.io/publication/sasfalsealarm/","publishdate":"2019-10-21T18:45:22-07:00","relpermalink":"/publication/sasfalsealarm/","section":"publication","summary":"Sensor fusion algorithms allow the combination of many heterogeneous data types to make sophisticated decisions. In many situations, these algorithms give increased performance such as better detectability and/or reduced false alarm rates. To achieve these benefits, typically some system or signal model is given. This work focuses on the situation where the event signal is unknown and a false alarm criterion must be met. Specifically, the case where data from multiple passive infrared (PIR) sensors are processed to detect intrusion into a room while satisfying a false alarm constraint is analyzed. The central challenge is the space of intrusion signals is unknown and we want to quantify analytically the probability of false alarm. It is shown that this quantification is possible by estimating the background noise statistics and computing the Mahalanobis distance in the frequency domain. Using the Mahalanobis distance as the decision metric, a threshold is computed to satisfy the false alarm constraint.","tags":[],"title":"Sensor Fusion for Intrusion Detection Under False Alarm Constraints","type":"publication"},{"authors":["Matthew Pugh"],"categories":[],"content":"","date":1426988390,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1426988390,"objectID":"323c8261ff373c83e8bd6b5110a7411b","permalink":"http://rexyroo.github.io/publication/sasminimax/","publishdate":"2019-10-21T18:39:50-07:00","relpermalink":"/publication/sasminimax/","section":"publication","summary":"The goal of sensor fusion is to combine the information obtained by various sensors to make better decisions. By better, it is meant that the sensor fusion algorithm provides, for example, better detectability or lower false alarm rates compared to decisions based upon a single sensor. This work is motivated by combining the data gathered by multiple passive infrared (PIR) sensors to detect intrusions into a room. Optimal decision theoretic approaches typically include statistical models for both the background (non-event) data, and intrusion (event) data. Concurrent work by the author has shown that by appropriately processing multiple PIR data streams, a statistic can be computed which has a known distribution on the background data. If the distribution of the statistic during an event is known, optimal decision procedures could be derived to perform sensor fusion. It is shown, however, that it is difficult to statistically model the event data. This paper thus focuses on using minimax theory to derive the worst-case event distribution for minimizing Bayes risk. Because of this, using the minimax distribution as a surrogate for the unknown true distribution of the event data provides a lower bound on risk performance. The minimax formulation is very general and will be used to consider loss functions, the probability of intrusions events and consider nonbinary decisions.","tags":[],"title":"A Minimax Approach to Sensor Fusion for Intrusion Detection","type":"publication"},{"authors":[],"categories":[],"content":" More On Run Differential In the previous post post, I looked at run differential from a correlation point of view. The correlation analysis is not very predictive however, unless we want to assume there is a linear relationship between normalized run differential and end-of-season run differential (which there could be, but the analysis seems to suggest the joint distribution is not Gaussian in which case the correlation is difficult to interpret). In this post, I\u0026rsquo;ll take a Bayesian approach to run differential that will be more predictive in nature. Namely, I will assume that the per game run differential (not normalized) is a discrete random variable. I will use the historical data on run differential to generate a prior and then use the games played by the A\u0026rsquo;s this season to estimate the probabilities of the discrete random variable. With those parameters estimated, I can compute the distribution of run differential at the end of the season. Additionally, it is straight forward to convert the multinomial run differential distribution into a binomial win distribution. In this way, I will also compute the distribution of the number of wins at the of the season. Prior Run Differential Distribution From the run differential data parsed in the previous post, it is straight forward to compute the run differential distribution from all the games from 1962 to 2013. Below is the distribution:\nAs is expected, the run differential is symmetric since when a team wins by, say, x runs, another team loses by x runs. I will model the run differential as a discrete random variable, i.e. let $X_{i} \\in {-30,\\ldots,30}$ be the run differential random variable for game $i$ and all the games will be i.i.d. realizations from this distribution. The range from -30 to 30 was chosen arbitrarily but covers all observed run differentials from 1962 onward. I will try to estimate the probabilities for each run differential value for the 2014 A\u0026rsquo;s. To do this, I will take a Bayesian approach and put a prior probability on the probabilities. Because the parameters are probabilities, we need a prior distribution on discrete probability distributions. Luckily, the Dirichlet distribution is exactly what is needed. Because the run differential random variable $X_{i}$ has support on ${-30,\\ldots,30}$, there are 61 hyperparameters for the Dirichlet distribution. I want the expectation of the Dirichlet prior distribution to be equal to the historical run differential distribution in the figure above, thus the hyperparameters are proportional to the probabilities in the above graph. I will play around with the proportionality constant later.\nBayesian Run Differential Analysis As stated earlier, I will model the run differential as a discrete random variable with parameters ${\\theta_{-30},\\ldots,\\theta_{30}}$ (the probabilities for each run differential value) and a prior distribution on $\\mathbf{\\theta}$ (the vector of $\\theta$ values) which is a Dirichlet distribution with hyperparameter $\\alpha$ (the vector of $\\alpha$ values). Let $\\mathcal{D}_{n}$ be the observed run differential for the 2014 A\u0026rsquo;s through $n$ games. Thus, for estimating the ${\\theta_{i}}$, we have the posterior distribution $p(\\theta \\vert \\mathcal{D}_{n})$, which by Bayes theorem is proportional to $p(\\mathcal{D}_{n} \\vert \\theta) p(\\theta \\vert \\alpha)$ where the likelihood $p(\\mathcal{D}_{n} \\vert \\theta)$ is a multinomial distribution and the prior $p(\\theta \\vert \\alpha)$ is the aforementioned Dirichlet distribution. The posterior in this case is well known (for example see Machine Learning: A Probabilistic Perspective Chapter 3); it is another Dirichlet distribution with parameters ${ \\tilde{\\alpha}_{-30},\\ldots, \\tilde{\\alpha}_{30}} = {\\alpha_{-30} + N_{-30},\\ldots,\\alpha_{30} + N_{30}}$ where $N_{i}$ is the number of times run differential $i$ is observed in $\\mathcal{D}_{n}$. Now that we know the posterior distribution $p(\\theta \\vert \\mathcal{D}_{n})$ is a Dirichlet distribution, we can compute the predictive posterior distribution $p(X = i \\vert \\mathcal{D}) = \\int p(X = i \\vert \\theta) p(\\theta \\vert \\mathcal{D}_{n}) d \\theta$, i.e. the probability that the run differential takes on value $i$ given the data marginalizing over the parameters. It turns out that because the posterior $p(\\theta \\vert \\mathcal{D}_{n})$ is Dirichlet and that we model $p(X = j \\vert \\theta) = \\theta_{j}$, the previous integral has a well known solution (again see Chapter 3 of Machine Learning: A Probabilistic Perspective):\n\\begin{align} p(X = i \\vert \\mathcal{D}) = \\frac{\\alpha_j + N_j}{\\sum (\\alpha_i + N_i)} \\end{align}\nThus, given the A\u0026rsquo;s run differential through $n$ games, $\\mathcal{D}_n$, we can compute the probability that the A\u0026rsquo;s have run differential $j$ in a game: $p(X = j \\vert \\mathcal{D}_{n})$. Note this is the probability given our model and we only hope that the model is close to reality.\nUnweighted $\\alpha$ Previously, it was mentioned that in order for the Dirichlet prior to have expectation equal to the historical data, the ${\\alpha_{i}}$ must be proportional to the the historical run differential probabilities. In this section we will assume the ${\\alpha_{i}}$ are equal to those probabilities, i.e. the proportionality constant is 1. Since we know $p(X = i \\vert \\mathcal{D})$ from the above analysis, we can compute the run differential for the entire season by drawing random variables from that distribution. For example, using the 39 game threshold from the previous post, we can draw $162-39=123$ random variables from this distribution to estimate the A\u0026rsquo;s end-of-season run differential. The end-of-season run differential is the sum of the run differential random variables and the known run differential after 39 games. To compute the end-of-season run differential, I use one million monte carlo runs. I also performed the analysis at the season half way point - after 81 games. Below are the end-of-season run differential distributions:\nHere are the same distributions, but overlapping:\nWe can convert the run differential distribution into a win/loss distribution since whenever the run differential is greater than 0, the team wins and whenever the run differential is less than 0, the team loses. Thus, let $p = \\sum_{i\u0026gt;0} p(X = i \\vert \\mathcal{D})$ be the probability that the team wins, and $1-p$ be the probability that the team loses. Then we can replicate the prior analysis but rather than having a multinomial experiment, we have a binomial experiment. For the end-of-season run differential, we had to compute the sum of the multinomial random variables, which is why I resorted to monte carlo. In the win/loss case, the number of wins at the end of the season is known analytically and is the binomial distribution, so monte carlo is not required. Below is the distribution of the number of wins:\nThe mean number of wins under this model after the A\u0026rsquo;s first 39 games is $\\mu_{39} = 99.34$ with variance $\\sigma^{2}_{39} = 29.19$ and the mean number of wins after the A\u0026rsquo;s first 81 games is $\\mu_{81} = 101.87$ with variance $\\sigma^{2}_{81} = 18.92$. Note that a win/loss model could be derived directly in a similar way as the previous analysis using a binomial likelihood instead of a multinomial likelihood and a beta prior distribution rather than a Dirichlet prior.\nThe previous results assumed the hyperparameters $\\alpha$ were equal to the empirical historical run differential distribution. From before, we know that $p(X = i \\vert \\mathcal{D}) = \\frac{\\alpha_j + N_j}{\\sum (\\alpha_i + N_i)}$, thus all the $\\alpha_{i}$ are probabilities so the predictive posterior will be dominated by the observed run differentials $\\mathcal{D}_{n}$. Indeed this is the case, as observed in the following plot:\nAs expected, the Dirichlet prior is essentially irrelevant and the observed run differential dominates. In this case, the historical knowledge of run differential plays no role in the end-of-season analysis. In the next sections we will place different weights on the ${\\alpha_{i}}$ so the prior plays a greater role in the estimate of end-of-season run differential and the number of wins at the end of the season.\nFully Weighted $\\alpha$ In this section, we will take the opposite extreme, where the ${\\alpha_{i}}$ are the historical run differential counts. Because there are 10,000s of games, the prior will dominate and the A\u0026rsquo;s run differential counts for 39 and 81 games will be insignificant. Below is the predictive posterior distribution with the new ${\\alpha_{i}}$ hyperparameters and as expected, the prior dominates and makes the posterior equivalent to the prior:\nIn this case, history dominates and we are not using any information from the A\u0026rsquo;s current season. Below are the same plots as the previous section, but with the historically dominate prior:\nBecause the past dominates, and the A\u0026rsquo;s have a well above average run differential so far this year, the predictions are pessimistic. With unweighted $\\alpha$, the predicted end-of-season run differential was over 250 after both 39 and 81 games. With the dominate prior, the prediction has dropped to end-of-season run differentials of 62 and 135. Below is the end of season win distribution:\nAfter 39 games, the expected number of wins is $\\mu_{39} = 85.50$ with variance $\\sigma^{2}_{39} = 30.75$ and after 81 games, the expected number of wins is $\\mu_{81} = 91.50$ with variance $\\sigma^{2}_{81} = 20.25$. Once again comparing this to the previous section, we see the wins prediction is very pessimistic.\nModerately Weighted $\\alpha$ In this section, I will chose a constant $c$ to multiply the unweighted ${ \\alpha_{i} }$ (the historical probabilities from two section prior). For this section I will choose $c = 100$. Below are the predictive posteriors and historical prior under this weighting:\nWe see that this weighting, as expected, causes the posterior the be inbetween the unweighted and fully weighted posteriors from the prior sections - it is a compromise between the past historical data and the A\u0026rsquo;s current season. Below are the run differential distributions using this weighting:\nAgain, notice how the mean run differential predictions lie inbetween the current season dominated predictions of the unweighted ${\\alpha_{i}}$ and the historically dominated predictions of the fully weighted $ { \\alpha_{i} } $. This last figure is the wins distribution for this weighting:\nAfter 39 games, the expected number of wins is $\\mu_{39} = 89.48$ with variance $\\sigma^{2}_{39} = 30.62$ and after 81 games, the expected number of wins is $\\mu_{81} = 96.20$ with variance $\\sigma^{2}_{81} = 19.98$.\nConcluding Remarks The previous analysis shows some interesting Bayesian analysis we can perform with the run differential information. The art of Bayesian analysis is justifying the choice of the prior. We looked at three different Dirichlet priors in this article. It could be possible to select the hyperprior weightings by trying to reduce the mean squared error on some test set, but that will not be persued here (due to laziness).\n","date":1405814400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1405814400,"objectID":"573ee9518416f97885984dcbc0b9bc6a","permalink":"http://rexyroo.github.io/post/run-differential-2/","publishdate":"2014-07-20T00:00:00Z","relpermalink":"/post/run-differential-2/","section":"post","summary":"More On Run Differential In the previous post post, I looked at run differential from a correlation point of view. The correlation analysis is not very predictive however, unless we want to assume there is a linear relationship between normalized run differential and end-of-season run differential (which there could be, but the analysis seems to suggest the joint distribution is not Gaussian in which case the correlation is difficult to interpret).","tags":[],"title":"A Generative Model Approach to Run Differential","type":"post"},{"authors":[],"categories":[],"content":" Everyone\u0026rsquo;s Talking About Run Differential So it\u0026rsquo;s been a long time since I\u0026rsquo;ve attempted a blog post. My friend and co-worker EBHI is a moderator at the awesome A\u0026rsquo;s blog Athletic\u0026rsquo;s Nation. Growing up and now working in the East Bay, I\u0026rsquo;ve always been an A\u0026rsquo;s fan and so I thought it might be fun to do some baseball analysis. This season the A\u0026rsquo;s are doing great, and I\u0026rsquo;ve been reading a great deal about run differential. The first article chronologically I saw regarding run differential and the A\u0026rsquo;s was on Baseball Prospectus. Then mlb.com had an article regarding the A\u0026rsquo;s and run differential, and most recently, two eerily similar articles were posted within a day of each here and here. I originally had some grand aspirations to use some fancy correlation metrics, but that will have to wait for a possible future post. In fact the analysis in this article is pretty elementary, but it does provide good python practice for me. In this analysis I will only be focusing on classical correlation coefficients. Correlating Run Differential with Various Things The first thing I wanted to do was reproduce the same figure found in the Baseball Prospectus article. In order to do that I needed to download the data for all games from the 1962 to 2013 seasons and parse them to compute the game-by-game run differential. I download the data from here. The run differential is normalized by the game number (e.g. the run differential after game 81 is divded by 81). Here\u0026rsquo;s the really simple code I used to get the files:\nimport urllib base_url = 'http://www.retrosheet.org/gamelogs/' for year in range(1962,2014): url = base_url + 'gl' + str(year) + '.zip' #Change Directory Accordingly directory = '' filename = directory + 'gl' + str(year) + '.zip' testfile=urllib.URLopener() testfile.retrieve(url,filename)  With all the data downloaded and processed, I was able to reproduce the figure:\nComparing this to the Baseball Prospectus article, we see that they are visually identical. The analysis from the article uses a correlation value of 0.7 as being significant, and this threshold is exceeded at the 39th game. I will provide further analysis later in this article. Next I wanted to compute the correlation between normalized run differential and end-of-season winning percentage. I also included the above plot of the correlation between the game-by-game normalized run differential and end-of-season run differential. Because it\u0026rsquo;s easy enough to include, I also plotted the game-by-game winning percentage correlated with end-of-season winning percentage. Below is the correlation plot:\nThe above plot shows that the correlation trends are very similar, but normalized run differential is less correlated with end-of-season winning percentage (red curve) than end-of-season normalized run differential (the blue curve, which seems expected). The black curve is the game-by-game winning percentage correlated with end-of-season winning percentage. Next I wanted to see if game-by-game run differential or game-by-game winning percentage correlated with the end-of-season rank. Below is the corresponding plot:\nThe above plot shows that both normalized run differential and winning percentage do not correlate with end-of-season rankings. The main take away thus far seems to be the following:\n game-by-game normalized run differential correlates well with end-of-season run differential after a sufficient number of games game-by-game winning percentage correlates well with end-of-season winning percentage after a sufficient number of games game-by-game normalized run differential correlates well with end-of-season winning percentage after a sufficient number of games game-by-game normalized run differential does not correlate with end-of-season rank game-by-game winning percentage does not correlate with end-of-season rank  Gaussian Analysis of Normalized Run Differential Just because two features appear correlated does not necessarily tell us much about the relationship between the two features. In this section I will look at the normalized run differential data and see if it appears gaussian. Below is the scatter plot of the normalized run-differential at the 39th game (the game mentioned in the Baseball Prospectus article where the correlation exceeds 0.7) as well as the whitened version of the data:\nThe plot on the left is the scatter plot of end-of-season normalized run differential versus normalized run differential for game 39 in the season. It looks like it could be a multivariate gaussian. I whitened the data and the scatter plot is on the right. It should appear \u0026ldquo;spherical\u0026rdquo; after the whitening and it is hard to tell but seems like it could be. Below are the marginal histograms of the normalized run differential after game 39 and the fitted normal distribution and the histogram of the end-of-season normalized run differential and fitted histogram:\nRunning a normality test on the marginals, the p-value for the x-axis marginal is 0.55 and the p-value for the y-axis marginal is 0.008. The null hypothesis is that the data is normal, and we would reject the null hypothesis at a p-value less than 0.05 typically. Thus it seems unlikely that the y-axis data (end-of-seasson normalized run differential) is normally distributed. Below are the Q-Q plots:\nThe above Q-Q plots seem confirm the results of the hypothesis tests: the marginal x distribution appears pretty normal but the marginal y distribution seems skewed. For fun, below are the marginal histograms, fitted normals and Q-Q plots of the whitened data:\nThe p-values for the marginal distribution normality tests after whitening are 0.55 and 0.61. Thus the rotated data \u0026ldquo;appears\u0026rdquo; more normal than the original data. The Q-Q plots also seem to indicate this. There do exist hypothesis tests for multivariate normality, but I\u0026rsquo;ve been lazy and did not implement them. If the data were truly bivariate normal, the data should appear guassian before and after the whitening process. Because it is the marginal y-axis distribution, which is the marginal distribution of the end-of-season normalized run differential, is the one that does not appear gaussian, none of the games can be jointly normal. Because of this, we should be careful is our intepretation of the correlation coefficient. The correlation coefficient measures linear relationships between two random variables, and is equal to the correlation coefficient of a bivariate normal when the data is jointly normal. Because it does not appear that the data is jointly normal, the correlation coefficient cannot be interpreted as the correlation coefficient of a bivariate normal.\nWe can repeat this experiment with the correlation between normalized run differential after 39 games and end-of-season winning percentage. The results are similar. Below are the scatter plots, marginal histograms and Q-Q plots:\nThe p-values for the hypothesis test are 0.55 and 0.0008. The p-value for the marginal x-distribution should be the same as before as it\u0026rsquo;s the same random variable. The p-value of the marginal end-of-season winning percentage seems very unlikely to be drawn from a Gaussian distribution, and thus joint will not be Gaussian.\nTo conclude this analysis, the correlation between normalized run differential for each game and either end-of-season normalized run differential or end-of-season winning percentage definitely seems to exists, but the relationship is more complicated as it does not appear that the data is normal. Future analysis would be required to determine or model the nature of this relationship.\n","date":1405728000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1405728000,"objectID":"e4a79442b0cc400e9ec957f2abf89df1","permalink":"http://rexyroo.github.io/post/run-differential-1/","publishdate":"2014-07-19T00:00:00Z","relpermalink":"/post/run-differential-1/","section":"post","summary":"Everyone\u0026rsquo;s Talking About Run Differential So it\u0026rsquo;s been a long time since I\u0026rsquo;ve attempted a blog post. My friend and co-worker EBHI is a moderator at the awesome A\u0026rsquo;s blog Athletic\u0026rsquo;s Nation. Growing up and now working in the East Bay, I\u0026rsquo;ve always been an A\u0026rsquo;s fan and so I thought it might be fun to do some baseball analysis. This season the A\u0026rsquo;s are doing great, and I\u0026rsquo;ve been reading a great deal about run differential.","tags":[],"title":"A Correlation Analysis of Run Differential","type":"post"},{"authors":[],"categories":[],"content":" Main Idea In the previous post, I was trying to solve the Markowitz portfolio optimization problem under the no shorts constraint:\n\\begin{align} \u0026amp;\\underset{\\mathbf{x}}{\\mbox{minimize}} \u0026amp; \u0026amp; \\mathbf{x}^{T} \\Sigma \\mathbf{x} \\\n\u0026amp; \\mbox{subject to} \u0026amp; \u0026amp; \\bar{\\mathbf{p}}^{T} \\mathbf{x} \\geq r_{\\min} \\\n\u0026amp; \u0026amp; \u0026amp;\\mathbf{1}^{T} \\mathbf{x} = 1 \\\n\u0026amp; \u0026amp; \u0026amp;\\mathbf{x} \\geq 0 \\end{align}\nwhere $\\mathbf{x}$ is the $n$-dimensional vector given our positions in $n$ assests, $\\bar{\\mathbf{p}}$ is the vector of mean returns for the $n$ assets and $\\Sigma$ is the covariance matrix for the $n$ assets. In this post, I am going to flip the role of the objective function and the mean return constraint in the above optimization problem, i.e. I want to maximize the mean return under a variance constraint. The new optimization problem is\n\\begin{align} \u0026amp;\\underset{\\mathbf{x}}{\\mbox{minimize}} \u0026amp; \u0026amp; \\bar{\\mathbf{p}}^{T} \\mathbf{x} \\\n\u0026amp; \\mbox{subject to} \u0026amp; \u0026amp; \\mathbf{x}^{T} \\Sigma \\mathbf{x} \\leq \\sigma_{\\max} \\\n\u0026amp; \u0026amp; \u0026amp;\\mathbf{1}^{T} \\mathbf{x} = 1 \\\n\u0026amp; \u0026amp; \u0026amp;\\mathbf{x} \\geq 0 \\end{align}\nwhere $\\sigma_{max}$ is the maximum variance constraint. The original Markowitz optimization problem was a quadratic program (QP), where as the new optimization problem is a quadratically constrained quadratic program (QCQP). QP is a subclass of QCQP, so as one would imagine, it is more difficult to solve QCQP, but luckily CVXOPT is up to the task after we convert the QCQP into an even more general class of optimization problems - second-order cone programs (SOCP). QCQPs to SOCPs CVXOPT does not have an explicit solver for QCQPs, rather we must convert the QCQP to a SOCP, or one could also convert it to a semidefinite program (SDP), which is more general than a SOCP. Note that if one is using MATLAB, there are many useful parsers that will directly solve the QCQP by doing the conversion to SOCPs or SDPs behind the scenes. Such parsers include CVX and YALMIP. Unfortunely, using python, our options are currently limited (to my knowledge), so we must make do with using CVXOPT and doing the conversion ourselves without the aid of a parser. The general form of a SOCP, as given in Boyd and Vandenberghe, is\n\\begin{align} \u0026amp;\\underset{\\mathbf{x}}{\\mbox{minimize}} \u0026amp; \u0026amp; \\mathbf{f}^{T} \\mathbf{x} \\\n\u0026amp; \\mbox{subject to} \u0026amp; \u0026amp; \\Vert A_{i} \\mathbf{x} + \\mathbf{b}_{i} \\Vert_{2} \\leq \\mathbf{c}_{i}^{T} \\mathbf{x} + d_{i} \\\n\u0026amp; \u0026amp; \u0026amp; F \\mathbf{x} = \\mathbf{g} \\\n\\end{align}\nThis may not immediately look like the optimization problem that we want to solve, but the conversion is straight forward. Handling the variance constraint first for $i=1$, let $A = \\Sigma^{\\frac{1}{2}}$, $\\mathbf{b}_{1} = 0$, $\\mathbf{c}_{1} = 0$ and $d_{i} = \\sqrt{\\sigma_{max}}$. Next, let us handle the equality constraints $F\\mathbf{x} = \\mathbf{g}$. The only equality constraint we need is $\\mathbf{1}^{T}\\mathbf{x} = 1$, thus we set $\\mathbf{F} = \\mathbf{1}^{T}$ and $\\mathbf{g} = 1$. Let $n$ be the number of assets, then we will need another $n$ inequality constraints of the form $\\Vert A_{i} \\mathbf{x} + \\mathbf{b}_{i} \\Vert_{2} \\leq \\mathbf{c}_{i}^{T} \\mathbf{x} + d_{i}$ for $i = 2,\\ldots,n+1$ to capture the constraint $\\mathbf{x} \\geq 0$. Namely, for $i = 2,\\ldots,n+1$, let $A_{i} = 0$, $\\mathbf{b}_{i} = 0$, $d_{i} = 0$ and $\\mathbf{c}_{i} = \\mathbf{e}_{i}$ where $\\mathbf{e}_{i-1}$ is the $\\mathbf{e}_{j}$ is the $j^{th}$ canonical basis vector.\nUnfortunately, we are not finished yet, because CVXOPT requires a specific format as specified in its documentation. The conversion from the above SOCP to the CVXOPT required format is described here. Following their procedure yields the following set of matrices as inputs into CVXOPT:\n\\begin{align} \u0026amp; G_{1} = \\left [ \\begin{array}{c} 0 \\\\ -\\Sigma^{\\frac{1}{2}} \\end{array} \\right ] \u0026amp; \\mbox{ and } \u0026amp; \\mathbf{h}_{1} = \\left [ \\begin{array}{c} \\sqrt{\\sigma_{max}} \\\\ \\mathbf{0} \\end{array} \\right ] \\\n\u0026amp; G_{i} = \\left [ -\\mathbf{e}_{i}^{T} \\right ] \u0026amp; \\mbox{ and } \u0026amp; \\mathbf{h}_{i} = \\left [ 0 \\right ] \u0026amp; \\mbox{ for } i = 2,\\ldots,n+1 \\end{align}\nwith the additional equality constraints described above. When implementing the prior SOCP, errors were encountered because apparently the method to generate the covariance matrix does not guarantee a positive semi-definite $\\Sigma$, which is required of a QCQP or SOCP. Because this is more an exercise in implementing ideas in python than to implementing perfect algorithms, I just made a quick fix: I project the $\\Sigma$ matrix onto the positive semi-definite cone by making all the negative eigenvalues in $\\Sigma$ equal to 0 (aside: in retrospect, this should be done in the previous post as well). Thus the final implementation of the SOCP for this problem is the following additional method to my MarkowitzOpt class:\ndef FlipConstraintMarkowitzOpt(meanvec,varvec,covvec,irate,varmax): '''Solve QCQP by converting to SOCP Format from: http://cvxopt.org/userguide/coneprog.html#second-order-cone-programming''' # Generate mean return vector pbar = np.append(meanvec.values,irate) # Get Values from varvec and covvec varvec = varvec.values covvec = covvec.values # Generate covariance matrix counter = 0 numPOS = pbar.size SIGMA = np.zeros((numPOS,numPOS)) for i in np.arange(numPOS-1): for j in np.arange(i,numPOS-1): if i == j: SIGMA[i,j] = varvec[i] else: SIGMA[i,j] = covvec[counter] SIGMA[j,i] = SIGMA[i,j] counter+=1 # Compute A matrix in optimization # A is the square root of SIGMA U,V = np.linalg.eig(SIGMA) # Project onto PSD U[U\u0026lt;0] = 0 Usqrt = np.sqrt(U) A = np.dot(np.diag(Usqrt),V.T) # Generate G and h matrices G1temp = np.zeros((A.shape[0]+1,A.shape[1])) G1temp[1:,:] = -A h1temp = np.zeros((A.shape[0]+1,1)) h1temp[0] = np.sqrt(varmax)\tfor i in np.arange(numPOS): ei = np.zeros((1,numPOS)) ei[0,i] = 1 if i == 0: G2temp = [matrix(-ei)] h2temp = [matrix(np.zeros((1,1)))] else: G2temp += [matrix(-ei)] h2temp += [matrix(np.zeros((1,1)))] # Construct list of matrices Ftemp = np.ones((1,numPOS)) F = matrix(Ftemp) g = matrix(np.ones((1,1))) G = [matrix(G1temp)] + G2temp H = [matrix(h1temp)] + h2temp # Solve QCQP # solvers.options['show_progress'] = False # Passing in -matrix(pbar) since maximizing sol = solvers.socp(-matrix(pbar),Gq=G,hq=H,A=F,b=g) xsol = np.array(sol['x']) # return answer return xsol  Results Running the same script as in the previous post except for calling the new method FlipConstraintMarkowitzOpt and using a maximum variance parameter of $\\sigma_{max} = 0.001$ yields the following figures:\nThe interesting thing about this project is to see how the algorithm distributes the proportion of money across assets. As with the previous project, I want to reiterate that this is not meant as an investigation into legitimate trading procedure. For comparison, below is the distribution using Markowitz portolio optimization from the previous post.\n","date":1394841600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1394841600,"objectID":"e80f90c6d8a7c4c8acad9fdddc28b491","permalink":"http://rexyroo.github.io/post/dual-markowitz/","publishdate":"2014-03-15T00:00:00Z","relpermalink":"/post/dual-markowitz/","section":"post","summary":"Main Idea In the previous post, I was trying to solve the Markowitz portfolio optimization problem under the no shorts constraint:\n\\begin{align} \u0026amp;\\underset{\\mathbf{x}}{\\mbox{minimize}} \u0026amp; \u0026amp; \\mathbf{x}^{T} \\Sigma \\mathbf{x} \\\n\u0026amp; \\mbox{subject to} \u0026amp; \u0026amp; \\bar{\\mathbf{p}}^{T} \\mathbf{x} \\geq r_{\\min} \\\n\u0026amp; \u0026amp; \u0026amp;\\mathbf{1}^{T} \\mathbf{x} = 1 \\\n\u0026amp; \u0026amp; \u0026amp;\\mathbf{x} \\geq 0 \\end{align}\nwhere $\\mathbf{x}$ is the $n$-dimensional vector given our positions in $n$ assests, $\\bar{\\mathbf{p}}$ is the vector of mean returns for the $n$ assets and $\\Sigma$ is the covariance matrix for the $n$ assets.","tags":[],"title":"Variation on Markowitz Portfolio Optimization","type":"post"},{"authors":[],"categories":[],"content":" Overview Markowitz portfolio optimization is a technique that gives the optimal position in a set of investiments to minimize risk with a minimum expected return constraint. This idea is by no means new, as Markowitz introduced the idea in 1952 and was awarded the Nobel prize in 1990 for his work on modern portfolio theory. Markowitz portfolio optimization requires that some statistics about the assests are known: the mean returns and covariance among the assests. It is unclear how to optimally estimate these parameters. When taking the coursera.org course on financial engineering from Columbia, there was a lecture on Markowitz portfolio optimization as well as a homework exercise, but these parameters were given. This positing is not about surveying different estimation techniques, nor proposing any new ones, but is rather an excuse for me to try to improve my python skills. Gathering The Data The first step is to get data on our assets. Here, I will be focusing on stocks. To keep things simple, I just selected five stocks: Apple, IBM, Google, Microsoft and Qualcomm. I will also consider a fixed interest rate position with zero risk. The pandas library provides an easy way to get all the stock prices at the close of every business day. The following code will produce a pandas Dataframe object with the daily adjusting closing price of the five stocks from January 1st, 2000 to January 1st, 2014. Since Google was not a publicly traded company during the beginning of this time frame, NaN are filled in. The list comprehension expression to get the data was taken from the book Python for Data Analysis by Wes McKinney.\nimport pandas as pd import numpy as np import pandas.io.data as web from pandas import Series, DataFrame StockList = ['AAPL','IBM','MSFT','GOOG','QCOM'] all_data = {} # For simplicity, assume fixed interest rate interest_rate = 0.03/12. # Minimum desired return rmin = 0.02 for ticker in StockList: all_data[ticker] = web.get_data_yahoo(ticker,'1/1/2000','1/1/2014') price = DataFrame({tic: data['Adj Close'] for tic, data in all_data.iteritems()})  print price.head(10)   AAPL GOOG IBM MSFT QCOM Date 2000-01-03 26.90 NaN 96.60 42.59 77.19 2000-01-04 24.63 NaN 93.32 41.15 69.77 2000-01-05 24.99 NaN 96.60 41.58 67.35 2000-01-06 22.83 NaN 94.93 40.19 60.30 2000-01-07 23.91 NaN 94.52 40.72 64.57 2000-01-10 23.49 NaN 98.26 41.01 68.61 2000-01-11 22.29 NaN 99.10 39.96 62.18 2000-01-12 20.95 NaN 99.51 38.66 59.52 2000-01-13 23.25 NaN 98.47 39.39 61.62 2000-01-14 24.13 NaN 99.61 41.01 60.46 [10 rows x 5 columns]  Estimating The Parameters Now that we have the stock data we need to figure out how to compute the required parameters for the Markowitz portfolio optimization: the mean returns and covariance. I also need to pick a time frame to compute these parameters over. In this case, I choose 20 business days. Thus, the first step is to compute the returns 20 days into the future:\n# Specify number of days to shift shift = 20 # Compute returns over the time period specified by shift shift_returns = price/price.shift(shift) - 1  We could also have used the pandas Dataframe method pct_change with the period set to 20. Now that we have the returns over the period we want, we can compute the mean and covariance. As stated before, the purpose of this exercise is more for me to improve at python and not particularly to do anything advanced, so I opted to use a exponential weighting or an AR-1 model:\n# Specify filter \u0026quot;length\u0026quot; filter_len = shift shift_returns_mean = pd.ewma(shift_returns,span=filter_len) shift_returns_var = pd.ewmvar(shift_returns,span=filter_len) # Compute covariances NumStocks = len(StockList) CovSeq = pd.DataFrame() for FirstStock in np.arange(NumStocks-1): for SecondStock in np.arange(FirstStock+1,NumStocks): ColumnTitle = StockList[FirstStock] + '-' + StockList[SecondStock] CovSeq[ColumnTitle] = pd.ewmcov( shift_returns[StockList[FirstStock]], shift_returns[StockList[SecondStock]], span=filter_len)  To reiterate, I am not claiming this is the best way of estimating the parameters.\nComputing The Markowitz Optimal Portfolio Now that we have the parameters we need, all that is left to do is to compute the optimal Markowitz portfolio. Let $\\mathbf{x}$ be the vector describing the proportion of money to put into each assest, $\\bar{\\mathbf{p}}$ be the vector of mean returns, $\\Sigma$ be the covariance matrix and $r_{\\min}$ be the minimum expected return that is desired. Having defined those variables, the optimal Markowitz portfolio assuming no short positions is given by the following quadratic program (see Boyd and Vandenberghe) \\begin{align} \u0026amp;\\underset{\\mathbf{x}}{\\mbox{minimize}} \u0026amp; \u0026amp; \\mathbf{x}^{T} \\Sigma \\mathbf{x} \\\n\u0026amp; \\mbox{subject to} \u0026amp; \u0026amp; \\bar{\\mathbf{p}}^{T} \\mathbf{x} \\geq r_{\\min} \\\n\u0026amp; \u0026amp; \u0026amp;\\mathbf{1}^{T} \\mathbf{x} = 1 \\\n\u0026amp; \u0026amp; \u0026amp;\\mathbf{x} \\geq 0 \\end{align} In order to solve this problem, I downloaded the python convex optimization package CVXOPT and then wrote the following class and method:\nfrom cvxopt import matrix, solvers import numpy as np def MarkowitzOpt(meanvec,varvec,covvec,irate,rmin): '''Framework and variable names taken from pg.155 of Boyd and Vandenberghe CVXOPT setup taken from: http://cvxopt.org/userguide/coneprog.html#quadratic-programming http://cvxopt.org/userguide/coneprog.html#quadratic-programming''' # Number of positions # Additional position for interest rate numPOS = meanvec.size+1 # Number of stocks NumStocks = meanvec.size # mean return vector pbar = matrix(irate,(1,numPOS)) pbar[:numPOS-1]=matrix(meanvec) # Ensure feasability Code pbar2 = np.array(pbar) if(pbar2.max() \u0026lt; rmin): rmin_constraint = irate else: rmin_constraint = rmin; counter = 0 SIGMA = matrix(0.0,(numPOS,numPOS)) for i in np.arange(NumStocks): for j in np.arange(i,NumStocks): if i == j: SIGMA[i,j] = varvec[i] else: SIGMA[i,j] = covvec[counter] SIGMA[j,i] = SIGMA[i,j] counter+=1 # Generate G matrix and h vector for inequality constraints G = matrix(0.0,(numPOS+1,numPOS)) h = matrix(0.0,(numPOS+1,1)) h[-1] = -rmin_constraint for i in np.arange(numPOS): G[i,i] = -1 G[-1,:] = -pbar # Generate p matrix and b vector for equality constraints p = matrix(1.0,(1,numPOS)) b = matrix(1.0) q = matrix(0.0,(numPOS,1)) # Run convex optimization program solvers.options['show_progress'] = False sol=solvers.qp(SIGMA,q,G,h,p,b) # Solution xsol = np.array(sol['x']) dist_sum = xsol.sum() return xsol  The input variable irate describes the interest rate which is assumed fixed during the duration of the optimizaton period (defined in this example above as 20 business days, however for simplicity I will assume it is fixed for all time). The interest rate is assumed to be a risk free position. The first part of the code creates a new mean vector with the interest rate return appended (note I assumed this value is the expected return for the duration of the optimization but it may be adjusted to account for this period, i.e. the return from interest over an arbitrary duration of time). The next part of the code converts the covariance matrix into the matrix structure required of CVXOPT. Then the quadratic program is put into the cannonical form required of CVXOPT and lastly the optimal position is computed and returned.\nPutting It All Together Now that the hard work is done, all that is left to do is actually compute the Markowitz optimal portfolio. I will start optimizing on January 3rd, 2006 (not at the beginning of our data set to allow some time for the means and covariance to stabilize a little), the the portfolio will be reoptimized every 20 business days.\n# Variable Initialization START_DATE = '2006-01-03' INDEX = shift_returns.index START_INDEX = INDEX.get_loc(START_DATE) END_DATE = INDEX[-1] END_INDEX = INDEX.get_loc(END_DATE) DATE_INDEX_iter = START_INDEX StockList.append('InterestRate') DISTRIBUTION = DataFrame(index=StockList) RETURNS = Series(index=INDEX) # Start Value TOTAL_VALUE = 1.0 RETURNS[INDEX[DATE_INDEX_iter]] = TOTAL_VALUE while DATE_INDEX_iter + 20 \u0026lt; END_INDEX: DATEiter = INDEX[DATE_INDEX_iter] # print DATEiter xsol = MarkowitzOpt(shift_returns_mean.ix[DATEiter], shift_returns_var.ix[DATEiter], CovSeq.ix[DATEiter],interest_rate,rmin) dist_sum = xsol.sum() DISTRIBUTION[DATEiter.strftime('%Y-%m-%d')] = xsol DATEiter2 = INDEX[DATE_INDEX_iter+shift] temp1 = price.ix[DATEiter2]/price.ix[DATEiter] temp1.ix[StockList[-1]] = interest_rate+1 temp2 = Series(xsol.ravel(),index=StockList) TOTAL_VALUE = np.sum(TOTAL_VALUE*temp2*temp1) # print TOTAL_VALUE # Increase Date DATE_INDEX_iter += shift # print 'Date:' + str(INDEX[DATE_INDEX_iter]) RETURNS[INDEX[DATE_INDEX_iter]] = TOTAL_VALUE # Remove dates that there are no trades from returns RETURNS = RETURNS[np.isfinite(RETURNS)]  Lastly, let us plot our results.\nimport matplotlib.pyplot as plt %matplotlib inline temp3 = DISTRIBUTION.T # To prevent cramped figure, only plotting last 10 periods ax = temp3.ix[-10:].plot(kind='bar',stacked=True) plt.ylim([0,1]) plt.xlabel('Date') plt.ylabel('Distribution') plt.title('Distribution vs. Time') ax.legend(loc='center left', bbox_to_anchor=(1, 0.5)) fig, axes = plt.subplots(nrows=2,ncols=1) price.plot(ax=axes[0]) shift_returns.plot(ax=axes[1]) axes[0].set_title('Stock Prices') axes[0].set_xlabel('Date') axes[0].set_ylabel('Price') axes[0].legend(loc='center left', bbox_to_anchor=(1, 0.5)) axes[1].set_title(str(shift)+ ' Day Shift Returns') axes[1].set_xlabel('Date') axes[1].set_ylabel('Returns ' + str(shift) + ' Days Apart') axes[1].legend(loc='center left', bbox_to_anchor=(1, 0.5)) fig.tight_layout() plt.figure() RETURNS.plot() plt.xlabel('Date') plt.ylabel('Portolio Returns') plt.title('Portfolio Returns vs. Time') plt.show()  This last plot shows us that using a our approach yielded a portfolio that increased 203%. This is obviously not to be taken too seriously as we know these stocks have done well over the past decade (surivivor bias).\n","date":1394323200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1394323200,"objectID":"41a0345b200ff2e62416dee18a45b9f1","permalink":"http://rexyroo.github.io/post/markowitz/","publishdate":"2014-03-09T00:00:00Z","relpermalink":"/post/markowitz/","section":"post","summary":"Overview Markowitz portfolio optimization is a technique that gives the optimal position in a set of investiments to minimize risk with a minimum expected return constraint. This idea is by no means new, as Markowitz introduced the idea in 1952 and was awarded the Nobel prize in 1990 for his work on modern portfolio theory. Markowitz portfolio optimization requires that some statistics about the assests are known: the mean returns and covariance among the assests.","tags":[],"title":"Markowitz Portfolio Optimization","type":"post"},{"authors":["Matthew Pugh","Bhaskar D. Rao"],"categories":[],"content":"","date":1350871099,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1350871099,"objectID":"7c3dd7d8a3fb1d1b5c15e6822f6c5a92","permalink":"http://rexyroo.github.io/publication/pfsasilomar/","publishdate":"2019-10-21T18:58:19-07:00","relpermalink":"/publication/pfsasilomar/","section":"publication","summary":"The proportional fair sharing (PFS) algorithm has been used in multi-user systems as an attempt to balance fairness and performance of the system throughput. Motivated by the cellular downlink scheduling problem, it is shown that when the rates of each user are i.i.d., the performance of the PFS scheduling algorithms is asymptotically equivalent to a purely greedy scheduling algorithm. The mean asymptotic throughput of the PFS algorithm is characterized and the rate of convergence to this limit is derived under i.i.d. models. Additionally the asymptotic covariance matrix about the convergence point is stated.","tags":[],"title":"The Proportional Fair Sharing Algorithm Under i.i.d. Models","type":"publication"},{"authors":["Derek Young","Jerry Brewer","Jeanette Chang","Tina Chou","Jacques Kvam","Matthew Pugh"],"categories":[],"content":"","date":1350870624,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1350870624,"objectID":"c5d7438dbb1d0ca77dd807c13fde3897","permalink":"http://rexyroo.github.io/publication/uvasilomar/","publishdate":"2019-10-21T18:50:24-07:00","relpermalink":"/publication/uvasilomar/","section":"publication","summary":"Communication using mid-ultraviolet radiation between 200nm and 280nm has received renewed attention due to advancements in UV LED emitters and unique propagation characteristics at these wavelengths. Atmospheric gases absorb light at mid-UV so that receivers or sensors operating on the earth’s surface receive no interference from solar radiation. This so-called “solar-blind” region of the spectrum allows the use of single-photon detection techniques. Further, UV light is strongly scattered by molecules in the air, enabling non-line-of-sight (NLOS) communication. We extend previous work in this area by incorporating angle-dependent Mie scattering into one of the standard propagation models, in an effort to include the effects of aerosols. Experimental results from outdoor measurements using a fog generator are also presented.","tags":[],"title":"Diffuse Mid-UV Communication in the Presence of Obscurants","type":"publication"},{"authors":["Matthew Pugh","Bhaskar D. Rao"],"categories":[],"content":"","date":1308625367,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1308625367,"objectID":"1d77db54a7fbb83b8018a261670cf888","permalink":"http://rexyroo.github.io/publication/asilomar2011/","publishdate":"2019-10-20T20:02:47-07:00","relpermalink":"/publication/asilomar2011/","section":"publication","summary":"To utilize the multi-user diversity in broadcast channels, the channel state information (CSI) of each user must be known at the transmitter. To reduce the overhead of CSI feedback under random beamforming the question of which receivers should feed back their CSI is investigated. Using the closed form expression for the SINR distribution, thresholding functions T(n) are designed to meet specific design criterion as a function of the number of receivers. Specifically three design criterion are proposed. The asymptotic limits of the successful thresholding functions T(n) are found. If T(n) scales slower than log n, asymptotically no performance is lost. If T(n) scales faster than log n, all multi-user diversity is lost.","tags":[],"title":"Feedback Reduction by Thresholding in Multi-User Broadcast Channels: Design and Limits","type":"publication"},{"authors":["Matthew Pugh"],"categories":[],"content":"","date":1303354644,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1303354644,"objectID":"0f40cf545adf79432be77d9037b7794b","permalink":"http://rexyroo.github.io/publication/thesis/","publishdate":"2019-10-20T19:57:24-07:00","relpermalink":"/publication/thesis/","section":"publication","summary":"","tags":[],"title":"Feedback Reduction Techniques and Fairness in Multi-User MIMO Broadcast Channels with Random Beamforming","type":"publication"},{"authors":["Matthew Pugh","Bhaskar D. Rao"],"categories":[],"content":"","date":1300676618,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1300676618,"objectID":"7b727d20085d5f2dc0ea432c928a00f6","permalink":"http://rexyroo.github.io/publication/dcc2011/","publishdate":"2019-10-20T20:03:38-07:00","relpermalink":"/publication/dcc2011/","section":"publication","summary":"Feedback of channel state information (CSI) in wireless systems is essential in order to exploit multi-user diversity and achieve the highest possible performace. When each spatially distributed user in the wireless system is assumed to have i.i.d. scalar CSI values, the optimal fixed-rate and entropy-constrained point density functions are established in the high-resolution regime for the quantization of the CSI feedback to a centralized scheduler under the mean square error (MSE) criterion. The spatially distributed nature of the users leads to a distributed functional scalar quantization approach for the optimal high resolution point densities of the CSI feedback. Under a mild absolute moment criterion, it is shown that with a greedy scheduling algorithm at the centralized scheduler, the optimal fixed-rate point density for each user corresponds to a point density associated with the maximal order statistic distribution. This result is generalized to monotonic functions of arbitrary order statistics. Optimal point densities under entropy-constrained quantization for the CSI are established under mild conditions on the distribution function of the CSI metric.","tags":[],"title":"Distributed Quantization of Order Statistics with Applications to CSI Feedback","type":"publication"},{"authors":["Matthew Pugh","Bhaskar D. Rao"],"categories":[],"content":"","date":1285038152,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1285038152,"objectID":"d2c99e65265f14917d2cba8bf31d4aae","permalink":"http://rexyroo.github.io/publication/icassp2010/","publishdate":"2019-10-20T20:02:32-07:00","relpermalink":"/publication/icassp2010/","section":"publication","summary":"In this paper we analyze the performance of random beamforming schemes in a multi-user Gaussian broadcast channel. Each user will have N  1 receive antennas allowing optimal combining to be performed. To notify the transmitter of its current channel state, each user feeds back their SINR after LMMSE combining. Two feedback schemes are analyzed. The first scheme allows each user to feedback the post-processed SINR for each of the random transmit beams. To analyze this scheme, the distribution of the post-processed SINR is found. The second scheme attempts to limit feedback by allowing each user to feedback only the largest observed post-processed SINR and the index of the associated transmit beam. Using the Fr´echet bounds, the throughput of the reduce feedback scheme is bounded and shown to have the same asymptotic scaling properties as the first scheme. Empirically, it is observed that as the number of users in the system increases, the reduced feedback scheme approaches the throughput of the scheme without thresholding.","tags":[],"title":"Feedback Reduction in MIMO Broadcast Channels with LMMSE Receivers","type":"publication"},{"authors":["Matthew Pugh","Bhaskar D. Rao"],"categories":[],"content":"","date":1269140577,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1269140577,"objectID":"e88dc6c71598b8d8e1a88400b15ea335","permalink":"http://rexyroo.github.io/publication/tsp2010/","publishdate":"2019-10-20T20:02:57-07:00","relpermalink":"/publication/tsp2010/","section":"publication","summary":"A random beamforming scheme for the Gaussian MIMO broadcast channel with channel quality feedback is investigated and extended. Considering the case where the n receivers each have N receive antennas, the effects of feeding back various amounts of signal-to-interference-plus-noise ratio (SINR) information are analyzed. Using the results from order statistics of the ratio of a linear combination of exponential random variables, the distribution function of the maximum order statistic of the SINR observed at the receiver is found. The analysis from viewing each antenna as an individual user is extended to allow combining at the receivers, where it is known that the linear MMSE combiner is the optimal linear receiver and the CDF for the SINR after optimal combining is derived. Analytically, using the Delta Method, the asymptotic distribution of the maximum order statistic of the SINR with and without combining is shown to be, in the nomenclature of extreme order statistics, of type 3. The throughput of the feedback schemes are shown to exhibit optimal scaling asymptotically in the number of users. Finally, to further reduce the amount of feedback, a hard threshold is applied to the SINR feedback. The amount of feedback saved by implementing a hard threshold is determined and the effect on the system throughput is analyzed and bounded.","tags":[],"title":"Reduced Feedback Schemes Using Random Beamforming in MIMO Broadcast Channels","type":"publication"},{"authors":["Matthew Pugh","Bhaskar D. Rao"],"categories":[],"content":"","date":1225248323,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1225248323,"objectID":"162188c483898d86b6fa115a1a6458eb","permalink":"http://rexyroo.github.io/publication/asilomar2008/","publishdate":"2008-10-28T19:45:23-07:00","relpermalink":"/publication/asilomar2008/","section":"publication","summary":"In this paper we analyze the performance of random beamforming schemes in a multi-user Gaussian broadcast channel utilizing SINR feedback. For generality, the receivers are allowed to have multiple receive antennas. The first scheme analyzed allows each receiver to feed back the largest SINR it observes for each transmit beam. The distribution function of the maximum SINR is derived and is noted to differ from previous work. In an effort to further reduce feedback, a scheme where each user feeds back the maximum SINR observed over its receive antennas and transmit beams. Using the Fr´echet bounds and properties of chi-squared random variables, the throughput of this system is bounded and empirically shown to approach the performance of the previous scheme as the number of users increases.","tags":[],"title":"On the Capacity of MIMO Broadcast Channels with Reduced Feedback by Antenna Selection","type":"publication"}]